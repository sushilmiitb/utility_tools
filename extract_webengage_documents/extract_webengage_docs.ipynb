{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acba0980-9f49-4770-8162-1872058616a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 185 links in the navbar.\n",
      "Saved: webengage_knowledge_base_text/Preface.txt\n",
      "Saved: webengage_knowledge_base_text/Users.txt\n",
      "Saved: webengage_knowledge_base_text/Events.txt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the main URL and the directory to store the pages\n",
    "MAIN_URL = \"https://knowledgebase.webengage.com/docs/preface\"\n",
    "SAVE_DIR = \"webengage_knowledge_base_text\"\n",
    "\n",
    "# Create the save directory if it doesn't exist\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize Selenium WebDriver (assumes ChromeDriver is in your PATH)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def fetch_article_text(url):\n",
    "    \"\"\"\n",
    "    Fetches the text content within the <article id=\"content\"> section of a given URL.\n",
    "    :param url: URL of the webpage to fetch\n",
    "    :return: Rendered text content within the specified <article> if successful, else None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(2)  # Wait for JavaScript to execute and content to load\n",
    "        # Parse the page source to find the <article> content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        article = soup.find('article', class_='rm-Article', id='content')\n",
    "        if article:\n",
    "            article_text = article.get_text(separator='\\n', strip=True)\n",
    "            return article_text\n",
    "        else:\n",
    "            print(f\"No article content found in {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_navbar_links(html):\n",
    "    \"\"\"\n",
    "    Extracts all the links from the navbar with id 'hub-sidebar'.\n",
    "    :param html: HTML content of the main page\n",
    "    :return: List of tuples containing link text and URL\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    navbar = soup.find('nav', id='hub-sidebar')\n",
    "    links = []\n",
    "\n",
    "    # If navbar exists, find all <a> tags within it, even if nested\n",
    "    if navbar:\n",
    "        for link in navbar.find_all('a', href=True):\n",
    "            span = link.find('span')\n",
    "            if span:\n",
    "                link_text = span.get_text(strip=True)\n",
    "                link_url = link['href']\n",
    "                if link_url.startswith('/'):\n",
    "                    link_url = \"https://knowledgebase.webengage.com\" + link_url\n",
    "                links.append((link_text, link_url))\n",
    "    \n",
    "    return links\n",
    "\n",
    "def save_text_content(title, content):\n",
    "    \"\"\"\n",
    "    Saves the text content of a page.\n",
    "    :param title: Title of the page (used as filename)\n",
    "    :param content: Rendered text content of the page\n",
    "    \"\"\"\n",
    "    # Sanitize title to use as a filename\n",
    "    filename = os.path.join(SAVE_DIR, f\"{title}.txt\")\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "def download_all_navbar_links(limit=3):\n",
    "    \"\"\"\n",
    "    Main function to download the rendered text content of linked pages.\n",
    "    :param limit: Number of pages to download for testing\n",
    "    \"\"\"\n",
    "    # Step 1: Get the main page HTML using Selenium for dynamic content\n",
    "    driver.get(MAIN_URL)\n",
    "    time.sleep(2)\n",
    "    main_page_html = driver.page_source\n",
    "\n",
    "    # Step 2: Extract links from the navbar\n",
    "    links = extract_navbar_links(main_page_html)\n",
    "    print(f\"Found {len(links)} links in the navbar.\")\n",
    "\n",
    "    # Step 3: Download a limited number of linked pages\n",
    "    for count, (title, url) in enumerate(links):\n",
    "        if count >= limit:\n",
    "            break\n",
    "        article_text = fetch_article_text(url)\n",
    "        if article_text:\n",
    "            save_text_content(title, article_text)\n",
    "        time.sleep(2)  # Adding a 2-second delay after each download\n",
    "\n",
    "# Run the script with a limit of 3 pages\n",
    "download_all_navbar_links(limit=3)\n",
    "\n",
    "# Close the Selenium WebDriver after scraping\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
